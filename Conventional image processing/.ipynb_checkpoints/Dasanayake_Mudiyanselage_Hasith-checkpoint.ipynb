{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "43cf7572",
   "metadata": {},
   "source": [
    "# Exercise sheet 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "408b398e",
   "metadata": {},
   "source": [
    "# 3) Q&A (Bonus Points)      Due: Fr, 09.06.23, 23:59\n",
    "\n",
    "#### (T3) Your task: Answer the questions below and give explanations where required. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e603a95",
   "metadata": {},
   "source": [
    "1. Describe the training of a Decision Tree (Split Criterion,Termination Criterion).\n",
    "\n",
    "Split Criterion: A decision tree is trained by recursively splitting the data based on certain criteria. Common split criteria include Gini impurity and information gain, which measure the homogeneity of the target variable within each split.\n",
    "\n",
    "Termination Criterion: The training process stops when certain termination criteria are met. These criteria can include reaching a maximum depth for the tree, having a minimum number of samples in each leaf node, or achieving a minimum improvement in impurity or information gain.\n",
    "2. Name 3 good reasons for using a Decision Tree as a Machine Learning Model.\n",
    "Interpretability: Decision trees provide easily interpretable rules that can be understood and explained by humans.\n",
    "\n",
    "Non-linear Relationships: Decision trees can capture non-linear relationships between features and the target variable.\n",
    "\n",
    "Handling Mixed Data: Decision trees can handle both numerical and categorical data without requiring extensive data preprocessing.\n",
    "\n",
    "3. Describe the data requirement and assumption for a classification with a Linear Discriminant Analysis (LDA) transform.\n",
    "\n",
    "Data Requirement: LDA requires labeled training data with known class labels.\n",
    "\n",
    "Assumption: LDA assumes that the data follows a multivariate Gaussian distribution, and the covariance matrices of the classes are equal.\n",
    "\n",
    "4. What is the objective of the LDA transform and how do the nominator and denominator of the Fishers Ratio (J in the lecture) relate to it?\n",
    "Objective: The objective of LDA is to find a linear transformation that maximizes the separation between classes while minimizing the variance within each class.\n",
    "\n",
    "Fishers Ratio: The numerator of the Fisher's ratio represents the between-class variance, and the denominator represents the within-class variance. Maximizing the Fisher's ratio aims to maximize the between-class variance relative to the within-class variance.\n",
    "\n",
    "5. Describe the optimization criterion that is used to fit the dividing hyper plane for Support Vector Machines (SVM).\n",
    "\n",
    "SVMs use the optimization criterion of maximizing the margin between the decision boundary (hyperplane) and the support vectors. This is achieved by solving a quadratic optimization problem.\n",
    "\n",
    "6. How is non-linear separability achieved with SVMs and why do we need the kernel trick?\n",
    "\n",
    "Non-linear separability: SVMs can handle non-linearly separable data by mapping the original feature space into a higher-dimensional space where the data becomes linearly separable.\n",
    "\n",
    "Kernel trick: The kernel trick allows SVMs to implicitly operate in this higher-dimensional space without explicitly computing the transformations. It avoids the computational burden of explicitly transforming the data while still capturing the non-linear patterns.\n",
    "\n",
    "7. How are the coefficients of a regression model estimated?\n",
    "\n",
    "Coefficient estimation in regression models is typically performed using techniques like ordinary least squares (OLS), which minimize the sum of squared residuals between the predicted and actual values.\n",
    "\n",
    "8. How is a regression model evaluated, and how can this evaluation be interpreted intuitively?\n",
    "\n",
    "Regression models are evaluated using metrics such as mean squared error (MSE), root mean squared error (RMSE), or coefficient of determination (R-squared). Lower values of MSE or RMSE indicate better model performance, while higher R-squared values indicate a better fit between the model and the data. These metrics can be interpreted as a measure of prediction accuracy or explained variance.\n",
    "\n",
    "9. Describe one method to combat overfitting for a regression model.\n",
    "\n",
    "Regularization techniques, such as ridge regression or LASSO, can be employed to combat overfitting in regression models. These techniques introduce penalty terms that restrict the model's complexity, preventing it from fitting the noise in the data too closely."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
